{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé¨ Text-to-Video Fine-Tuning + Upload + Space Demo\n",
        "\n",
        "This notebook installs libraries, loads data, loads a pretrained text-to-video pipeline, shows a **lightweight training scaffold**, then uploads your model **privately** to Hugging Face and provides a Space demo script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install diffusers transformers accelerate safetensors\n",
        "!pip -q install imageio[ffmpeg] huggingface_hub datasets gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, math, random\n",
        "import torch, torchvision\n",
        "from huggingface_hub import login, HfApi, Repository\n",
        "from datasets import load_dataset\n",
        "from diffusers import DiffusionPipeline\n",
        "import imageio\n",
        "print(\"Torch CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîë Login to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paste your Write token from https://huggingface.co/settings/tokens\n",
        "login(input(\"Enter your Hugging Face token (Write): \").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Load Dataset\n",
        "Upload a small zip of short videos (2‚Äì5s), or use a public dataset to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_UPLOAD = True  # Set False to use a public dataset example\n",
        "\n",
        "if USE_UPLOAD:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # e.g., dataset.zip\n",
        "    zip_name = list(uploaded.keys())[0]\n",
        "    !mkdir -p dataset\n",
        "    !unzip -o \"$zip_name\" -d dataset/\n",
        "    data_root = \"dataset\"\n",
        "else:\n",
        "    # Example public dataset placeholder\n",
        "    ds = load_dataset(\"damo-vilab/kinetics-mini\")\n",
        "    data_root = None  # Using hf dataset in memory\n",
        "print(\"Data ready:\", data_root or \"HF dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def extract_frames_from_videos(src_dir, dst_dir, fps=8, max_frames=16):\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    video_files = glob.glob(os.path.join(src_dir, \"**/*.mp4\"), recursive=True)\n",
        "    if not video_files:\n",
        "        print(\"No .mp4 files found in your dataset folder. Place short .mp4 clips in subfolders.\")\n",
        "        return []\n",
        "\n",
        "    extracted = []\n",
        "    for vid in video_files:\n",
        "        # Use imageio to read frames\n",
        "        reader = imageio.get_reader(vid)\n",
        "        frames = []\n",
        "        try:\n",
        "            for i, frame in enumerate(reader):\n",
        "                if len(frames) >= max_frames:\n",
        "                    break\n",
        "                frames.append(frame)\n",
        "        except Exception as e:\n",
        "            print(\"Error reading\", vid, e)\n",
        "        finally:\n",
        "            reader.close()\n",
        "\n",
        "        if not frames:\n",
        "            continue\n",
        "\n",
        "        # Save frames as PNGs\n",
        "        rel = os.path.relpath(vid, src_dir)\n",
        "        stem = Path(rel).with_suffix(\"\").as_posix().replace(\"/\", \"_\")\n",
        "        out_dir = os.path.join(dst_dir, stem)\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        for idx, fr in enumerate(frames):\n",
        "            Image.fromarray(fr).save(os.path.join(out_dir, f\"{idx:03d}.png\"))\n",
        "        extracted.append(out_dir)\n",
        "    return extracted\n",
        "\n",
        "if data_root:\n",
        "    extracted_dirs = extract_frames_from_videos(data_root, \"frame_dataset\")\n",
        "    print(\"Extracted clip dirs:\", extracted_dirs[:3], \"... total:\", len(extracted_dirs))\n",
        "else:\n",
        "    extracted_dirs = []  # Using HF dataset path (not extracted here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Load Pretrained Text-to-Video Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id = \"damo-vilab/text-to-video-ms-1.7b\"  # you can change\n",
        "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "pipe = pipe.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Lightweight Training Scaffold (LoRA-style placeholder)\n",
        "This is a **scaffold** to show where your training logic would go. True fine-tuning of large text-to-video models is compute-heavy and implementation-specific.\n",
        "\n",
        "We simulate a quick step so you can validate saving & pushing a model. Replace this with your real training when ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# Example: we'll pretend to update UNet parameters slightly\n",
        "params = [p for p in pipe.unet.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(params, lr=1e-6)\n",
        "\n",
        "pipe.unet.train()\n",
        "fake_loss = torch.tensor(0.0, requires_grad=True, device=device)\n",
        "(fake_loss + 0.0).backward()  # no-op backward to validate graph\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()\n",
        "pipe.unet.eval()\n",
        "\n",
        "print(\"Scaffold training step completed (placeholder). Replace with real loop for true fine-tuning).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ñ∂Ô∏è Test Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"A cute white cat dancing in space, cinematic, 4k\"\n",
        "out = pipe(prompt, num_frames=8)\n",
        "frames = out.frames\n",
        "os.makedirs(\"samples\", exist_ok=True)\n",
        "for i, fr in enumerate(frames):\n",
        "    imageio.imwrite(f\"samples/frame_{i:02d}.png\", fr)\n",
        "imageio.mimsave(\"samples/sample.mp4\", frames, fps=8)\n",
        "\"Generated samples at samples/sample.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save Fine-Tuned (or scaffold) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAVE_DIR = \"my_text2video_model\"\n",
        "pipe.save_pretrained(SAVE_DIR)\n",
        "print(\"Saved to\", SAVE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚òÅÔ∏è Push to Hugging Face (Private)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = HfApi()\n",
        "repo_name = \"my-text2video-model\"  # change if you like\n",
        "repo_url = api.create_repo(name=repo_name, private=True, exist_ok=True)\n",
        "repo = Repository(local_dir=repo_name, clone_from=repo_url)\n",
        "!cp -r my_text2video_model/* \"{repo_name}\"/\n",
        "# Minimal README for the model card\n",
        "with open(f\"{repo_name}/README.md\", \"w\") as f:\n",
        "    f.write(\"# My private text-to-video model\\n\\nThis model was prepared via Colab. Replace this text with usage instructions and sample outputs.\")\n",
        "!cd \"{repo_name}\" && git add . && git commit -m \"Upload model\" && git push\n",
        "print(\"Pushed to:\", repo_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Space Demo (Gradio)\n",
        "Copy `space_app/app.py` from this repo to your new Space.\n",
        "Set your model id in the UI or hardcode it in `app.py`. To monetize, enable **Paid Space** in Space Settings."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "text2video-finetune.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}